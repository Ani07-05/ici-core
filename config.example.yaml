embedders:
  sentence_transformer:
    batch_size: 32
    device: cuda
    model_name: all-MiniLM-L6-v2
ingestors:
  telegram:
    api_hash: "YOUR_TELEGRAM_API_HASH"
    api_id: "YOUR_TELEGRAM_API_ID"
    phone_number: "+1234567890"
    request_delay: 0.5
    session_string: "YOUR_TELEGRAM_SESSION_STRING"
pipelines:
  telegram:
    batch_size: 100
    schedule:
      interval_minutes: 1
preprocessors:
  telegram:
    chunk_size: 512
    include_overlap: true
    max_messages_per_chunk: 10
    time_window_minutes: 15
state_manager:
  db_path: ingestor_state.db
vector_stores:
  chroma:
    collection_name: telegram_messages
    embedding_function: sentence_transformer
    persist_directory: ./chroma_db
loggers:
  structured_logger:
    name: structured_logger
    level: DEBUG
    log_file: ./logs/structured_logger.log
    console_output: true
    user_betterstack: true
    source_token: "YOUR_SOURCE_TOKEN"
    host: "YOUR_INGESTION_HOST"

# Component configurations
validator:
  # Simplified validator that only checks if input is from COMMAND_LINE source
  allowed_sources: ["COMMAND_LINE"]
  rules: []
  
prompt_builder:
  template: "Context:\n{context}\n\nQuestion: {question}"
  fallback_template: "Answer based on general knowledge: {question}"
  error_template: "Unable to process: {error}"

# Generator configuration with support for different types
generator:
  # Common generator configurations
  
  # Provider type: "openai", "langchain", etc
  type: "langchain"
  
  # LangChain provider configurations (when type is "langchain")
  
  # Option 1: OpenAI via LangChain
  provider: "openai"
  model: "gpt-4o"
  api_key: "YOUR_OPENAI_API_KEY"
  
  # Option 2: Ollama via LangChain
  # Uncomment to use Ollama
  # provider: "ollama"
  # model: "llama3"  # Or another model you have in Ollama
  # base_url: "http://localhost:11434"  # Default Ollama URL
  
  # Chain and memory configuration
  chain_type: "simple"
  memory:
    type: "buffer"
    k: 5
    
  # Common generation options
  default_options:
    temperature: 0.7
    max_tokens: 1024
    top_p: 1.0
    frequency_penalty: 0.0
    presence_penalty: 0.0
  
  # Retry configuration
  max_retries: 3
  base_retry_delay: 1

orchestrator:
  num_results: 5
  similarity_threshold: 0.7
  rules_source: "config"
  pipeline:
    auto_start: true
    ingestor_id: "telegram"
  error_messages:
    validation_failed: "I'm unable to process your request due to security restrictions."
    no_documents: "I don't have specific information on that topic yet."
    generation_failed: "I'm having trouble generating a response right now. Please try again later."
  validation_rules:
    default: []  # Empty default rules - only checks if source is COMMAND_LINE
    # Can add user-specific rules here if needed:
    # user123: [{...}]
  user_context:
    default: {"permission_level": "user"}
    # Can add user-specific context here:
    # admin_user: {"permission_level": "admin"}
  generation_options:
    temperature: 0.7
    max_tokens: 1024